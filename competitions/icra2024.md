---
title: GPR2024
subtitle: Crowdsourced Map Association for Large-scale Environments
description: Crowdsourced Map Association for Large-scale Environments
layout: page
# showcase: showcase_example
show_sidebar: false
hide_footer: false
hero_height: is-large
hero_image: /img/web.gif
---

## Introduction

Accurate and current mapping of large-scale environments is of paramount importance for a wide range of applications, including urban planning, infrastructure management, environmental monitoring, and resource management. However, traditional mapping methods can be time-consuming, expensive, and struggle to keep up with the rapid pace of change in many areas. By leveraging the collective efforts of individuals equipped with LiDAR sensors, we can revolutionize the way we map and understand our world. Crowdsourced mapping allows us to rapidly collect and aggregate vast amounts of highly accurate 3D data, enabling the creation of detailed maps that would be impractical or prohibitively expensive using conventional methods.

<figure>
 <img src="/img/competition/icra_2024/map_merge.png" style="width:40%" />
</figure>

This competition aims to showcase the potential of crowdsourced mapping and foster innovation in this field. Participants will have the opportunity to contribute their skills and efforts to map large-scale environments, pushing the boundaries of what is possible and demonstrating the power of collaborative mapping.

## The Challenge

### Dataset

This challenge will take place primarily on [M2DGR](https://github.com/SJTU-ViSYS/M2DGR) Dataset. M2DGR isÂ a novel large-scale dataset collected by a ground robot with a full sensor-suite including six fish-eye and one sky-pointing RGB cameras, an infrared camera, an event camera, a Visual-Inertial Sensor (VI-sensor), an inertial measurement unit (IMU), a LiDAR, a consumer-grade Global Navigation Satellite System (GNSS) receiver and a GNSS-IMU navigation system with real-time kinematic (RTK) signals. The ground truth trajectories were obtained by the motion capture device, a laser 3D tracker, and an RTK receiver.

<figure>
 <img src="/img/competition/icra_2024/m2dgr.png" style="width:60%" />
</figure>

The dataset comprises 36 sequences (about 1TB) captured in diverse scenarios including both indoor and outdoor environments. There are many overlapping areas between these sequences, which facilitates resembling between trajectories. In addition, some trajectories were recorded in challenging scenarios like lifts, complete darkness, etc, which bring more challenges to this task.

### Schedule

| Date         | Event                       |
| ------------ | --------------------------- |
| May 15 2024  | Online Submission Open      |
| May 16 2024  | Release of Data             |
| June 20 2024 | Competition Deadline        |
| June 30 2024 | Award Decision Announcement |

### Awards

**1st Place**
Cash $3000 + CERTIFICATE

*This award will be given to one competitor.*

**2nd Place**
Cash $4000 + CERTIFICATE

*This award will be given to two competitors, an amount of $2000 will be given to each one.*

**3rd Place**
Cash $3000 +CERTIFICATE

*This award will be given to three competitors, an amount of $1000 will be given to each one.*

## Contact

If you have any questions, please contact [ryanzhao9459@gmail.com](mailto:ryanzhao9459@gmail.com).

## Invited Talkers

<table style="width:100%;">
  <tr>
    <td style="text-align: center;">
      <img src="/img/competition/icra_2024/zoudanping.jpeg" alt="Danping Zou" style="width:300px; height:300px; object-fit: contain; background-color: white;">
      <p><a href="https://scholar.google.com.sg/citations?user=y6FsLDQAAAAJ&hl=en" target="_blank"><strong>Danping Zou</strong></a><br>Shanghai Jiao Tong University</p>
    </td>
    <td style="text-align: center;">
      <img src="/img/competition/icra_2024/zhangfu.jpeg" alt="Fu Zhang" 
      style="width:300px; height:300px; object-fit: contain; background-color: white;">
      <p><a href="https://scholar.google.com.sg/citations?user=V-eYCF8AAAAJ&hl=en&oi=ao" target="_blank"><strong>Fu Zhang</strong></a><br>University of Hong Kong</p>
    </td>
    <td style="text-align: center;">
      <img src="/img/competition/icra_2024/gaofei.png" alt="Fei Gao" 
      style="width:300px; height:300px; object-fit: contain; background-color: white;">
      <p><a href="https://scholar.google.com.sg/citations?user=4RObDv0AAAAJ&hl=en&oi=ao" target="_blank"><strong>Fei Gao</strong></a><br>Zhejiang University</p>
    </td>
    <td style="text-align: center;">
      <img src="/img/competition/icra_2024/zhouboyu.jpeg" alt="Boyu Zhou" 
      style="width:300px; height:300px; object-fit: contain; background-color: white;">
      <p><a href="https://scholar.google.com/citations?user=-fnyGY4AAAAJ" target="_blank"><strong>Boyu Zhou</strong></a><br>Sun Yat-sen University</p>
    </td>
  </tr>
  <tr>
    <td style="text-align: center;">
      <img src="/img/competition/icra_2024/zhaohang.jpeg" alt="Hang Zhao" 
      style="width:300px; height:300px; object-fit: contain; background-color: white;">
      <p><a href="https://scholar.google.com/citations?user=DmahiOYAAAAJ&hl=en&oi=ao" target="_blank"><strong>Hang Zhao</strong></a><br>Tsinghua University</p>
    </td>
    <td style="text-align: center;">
      <img src="/img/competition/icra_2024/wangchen.jpeg" alt="Chen Wang" 
      style="width:300px; height:300px; object-fit: contain; background-color: white;">
      <p><a href="https://scholar.google.com/citations?user=vZfmKl4AAAAJ&hl=en&oi=ao" target="_blank"><strong>Chen Wang</strong></a><br>University at Buffalo</p>
    </td>
    <td style="text-align: center;">
      <img src="/img/competition/icra_2024/chenxie.png" alt="Xieyuanli Chen" style="width:300px; height:300px; object-fit: contain; background-color: white;">
      <p><a href="https://scholar.google.com/citations?user=DvrngV4AAAAJ&hl=en&oi=ao" target="_blank"><strong>Xieyuanli Chen</strong></a><br>National University of Defense Technology</p>
    </td>
    <td> <!-- Placeholder if no image/text is to align rows evenly -->
    </td>
  </tr>
</table>
